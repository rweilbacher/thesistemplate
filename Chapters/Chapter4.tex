% Chapter 1

\chapter{Analysis of how the architectures satisfy the key wants} % Main chapter title

\label{Chapter4} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}

%----------------------------------------------------------------------------------------
% REWRITE Feels a bit lacking and doesn't really get to the point (Probably because I haven't entirely found the point yet)

The two  architectures that will be analyzed are \keyword{hardware separated subsystems} and the \keyword{embedded safety hypervisor}. It has been established that both can conceptually solve the mixed criticality problem and now it is time to compare the non-functional criteria of the architectures in question. 

First of, the most crucial differences between the two architecture will be elaborated. Then it will be explored how these differences can help or hinder the satisfaction of the \keyword{key wants} of safety-critical device manufacturers established in chapter \ref{Chapter3}. 

\section{Crucial differences}
\subsection{Hardware or software}
Even though the hypervisor can be supported by hardware the separation is still fundamentally implemented in software. In the\gls{HSS} architecture on the other hand, separation is entirely realized through hardware, or rather lack of shared hardware. This does not mean that a device with the \gls{HSS} architecture does not contain software, only that the software is limited to the business components of the device.

% TODO Try to find sources for these claims and explain them more deeply
% NOTE Have I ever explained the difference between random and systematic faults
There are a number of differences between hardware and software developments that are relevant to this analysis.
\begin{itemize}
\item Software is cheaper to change than hardware. 
\item Hardware complexity is restricted by physical laws. Software complexity is only really limited by our ability to manage it.
\item Software is more likely to contain unfound systematic faults.
\item Software is more expensive to verify as a result.
\end{itemize}
\subsection{Specialized or general}
A hypervisor is a general software solution that is meant to be usable in a lot of cases with minor modification. It is often procured from an external company and not developed in-house specifically for a project.

The \gls{HSS} architecture is technically a very general methodology but each implementation is a specialized solution for one project or project line. This does not mean there is no overlap between different projects using hardware separated subsystems but that there is little room for reuse of the architecture implementation.

% TODO Is there more to add?
An advantage of a more general solution is that the one-time development costs can be spread across multiple projects, or in the case of third-party solutions, across multiple customers.   

\subsection{Distributed or centralized system} \label{distributed-or-centralized}
The \gls{HSS} architecture ultimately achieves its separation through being a distributed system. In contrast, a hypervisor is a centralized system with added separation between the partitions. In the special case of hardware separated subsystems the subsystems are typically also heterogeneous, which leads to a number of potential disadvantages.
% NOTE Verify these claims
\begin{enumerate}
\item Partition binaries need to be compiled differently.
\item Partitions have distinct bootstrapping processes.
\item Debugging the system as a whole is not possible.
\item The asynchronous nature of partitions, along with point three, can lead to errors that are very difficult to replicate.
\end{enumerate}
This does not mean debugging problems in hypervisor implementations are impossible, just that they can be feasibly addressed in a centralized system. The hypervisor as an entity can have a complete view of the system and therefore the developer can provide appropriate tooling to debug hypervisor partitions or even the hypervisor itself.

However, the 

\subsection{Cost of adding a partition}
The core of the \gls{HSS} architecture is that every partition is its own computation domain with separate resources. In the hypervisor architecture the most expensive partition to add is the second one, as it involves accommodating for the hypervisor in the first place. Any partition that is added beyond the second just requires the appropriate amount of memory and computational resources to be available. Naturally, there are limits to this as each partition also introduces some overhead in terms of kernel memory and process switching times.  Furthermore, a partition requires a certain amount of resources to even do useful work in the first place.

% TODO Consider the fact that if the hypervisor only supports operating systems there is also overhead associated with that

% NOTE This sounds like it is overall more expensive
So only looking at the final unit cost, adding a partition to a hypervisor is cheaper than adding a partition to a hardware separated subsystem. The more far reaching effects of this will be analyzed later.

\subsection{Proven in use status}
% NOTE Actually now that I think about it more, I'm not sure this is has any ramifications for quality
While hypervisors have already been deployed in safety-critical devices, the ind
* HSS is a very established method and HV is still new

\section{Safety and regulatory compliance \label{safety-analysis}}
 Let's move on to the first and most important want, safety and regulatory compliance. 
 \paragraph{Hardware redundancy}
 % TODO Fix terminology and think if this is even a separation architecture
 % TODO Make this a crucial difference and change refs to "safety-analysis" to point to that section
 % TODO Include point about the HV being a single point of failure
 % NOTE I am mixing redundancy with fault detection
 At the highest criticality levels redundancy is often required to reduce the devices criticality level. Alternatively, a second partition may supervise the first to detect erroneous output, even if it can't fully replace the first in case of a failure. With Moore's law coming to an end transistors have achieved staggeringly small sizes. One unintended consequence of this is that complex processors have become more prone to random errors \cite{Constantinescu.2003}. Consequently, this scenario is becoming more relevant.
 
 % TODO Make an architecture diagram highlighting my point
 A hypervisor can at most offer software redundancy with two partitions running distinct software that achieve the same functionality. These partitions can then act as a fallback, supervise the other partition or operate in tandem and vote on each output. Hardware separated subsystems are much more confident in achieving this safety as they can offer both hardware and software redundancy. 
 
However, this use case is different from the pure mixed criticality problem. If there is any type of redundancy, both partitions are still at least somewhat responsible for the safety functions of the device. But with mixed criticality we often talk about being able to integrate basically unverified convenience functionality, like a complex GUI running under Linux, into the device. That would not be plausible in this use case unless another partition is added and as has been stated earlier, adding another partition is more expensive in the \gls{HSS} architecture.

% NOTE This makes more sense here but kind of violates the want structure  because there is already a cost want
\paragraph{Certification cost}
% TODO Figure out the hardware software difference and finish this section.
Statistically speaking it is usually cheaper to verify a device's hardware than its software. Software  [Unfinished, because this claim is unclear]

% Is this a mute point?
Since the lowest level of the hypervisor needs to be reimplemented for every platform, the hypervisor also needs to be reverified for every platform. However, this does not need mean the previous efforts are irrelevant. A case on regression analysis can be built.
Furthermore, a project specific risk analysis also needs to be carried out regardless of the hypervisors certification status.

Each \gls{HSS} implementation needs to be verified from scratch but as stated in section \ref{HSS} providing sufficient evidence for separation should be much easier. The entire point of the architectures separation is that there is almost no avenue for influence to occur. Only through specifically implemented and controlled communication paths.
% NOTE Think about that last point

Ultimately the amount of control a system designer can exert on a device with the \gls{HSS} architecture, the fact that is hardware and not software and it separation mechanism should make a \gls{HSS} architecture cheaper to get approval for in most cases.

% TODO Add recertification costs here?

\paragraph{Impact of scalability on safety}
The scalability problems of the \gls{HSS} architecture have already been discussed but not their implications on safety. \keyowrd{Electromagnetic compatibility} or \gls{EMC} for short is quite important for safety-critical devices, especially for ones operating in explosive atmospheres. Additional processors require more power and more cables that can act as antennas. This introduces more \gls{EMC} challenges that need to be tackled by electromagnetic shielding and other methods. If they are not properly addresses they can cause high redesign costs at the end of the development cycle.

Additional hardware components also introduce more points for random failure and degradation, which may further reduce a \gls{HSS} device's safety with a growing amount of partitions.
% NOTE Maybe mention HVs multicore support

\paragraph{Additional safety features}
A hypervisor is a general software solution that can afford to have additional safety features along with its separation. By design it has a full view of the system and its events, a lot of implementations offer features like \keyword{Health monitoring} or \keyword{logging}. This can enable the device to detect and react to faults as they occur and positively impact its safety rating.

\paragraph{Conclusion}
The \gls{HSS} architecture provides the best possible safety and separation, even though this performance may degrade, if more subsystems are added. But ultimately a hypervisor only needs to be safe enough, not safer than \gls{HSS}. It is possible that hypervisor technology is not yet mature enough to compete in the most critical environments where absolute certainty is required. Therefore, if the hypervisor architecture is being considered for a future project it should be clarified, whether or not it can pass the certification. If it turns out to be sufficient however, the manufacturer can take advantage of its other benefits detailed in the following sections.


%----------------------------------------------------------------------------------------

\section{Lowest possible cost and keeping the schedule}
\paragraph{Development costs}
Unfortunately, there is no general quantitative analysis of the development costs associated with the two architectures. Hypervisor developers do not make their license costs, or even their license models, public, as they are subject to negotiation and dependent on other factors like amount of produced units among other things. Since the \gls{HSS} architecture is usually a custom solution its price is equally difficult to predict. Nevertheless, there are some arguments that can be made on the relative price differences. 

Hypervisors, whether written in-house or by a third-party, are meant to be general solutions, applicable in many projects. In contrast, the \gls{HSS} architecture is reimplemented for each device or device family. This gives the developers more control but general solutions have a significant benefit: The cost of the hypervisor will be spread across multiple projects, or in the case of a third-party hypervisor even across multiple customers. Another benefit of using a third-party hypervisor is that in a long-term relationship with the developer, prices may be favourable for the manufacturer.

But there is a problem with general solutions: It is possible that the manufacturer is forced to pay for functionality he would not have included in the device, had he used a specialized architecture. 

In the hypervisor architecture the manufacturer also needs to pay the potential license costs for the operating systems, running on the device, on top of the original hypervisor license costs.
 

% TODO Think about this point and incorporate it
* Mention how the cost not being known impacts the analysis. HV is really only interesting if it can be done cheaper than HSS and enable mixed criticality in different scenarios and projects. 

\paragraph{Hardware costs}
% TODO Read this again.
Each additional subsystem in the \gls{HSS} architecture will add another processor, memory and peripherals and consequently increase the device's hardware cost significantly. A hypervisor would usually require a more advanced processor with slightly more memory and computational power but this is less expensive in most cases [source Roy Master thesis?]. Additionally, most peripherals can be shared between hypervisor partitions with some exceptions.
* Higher Size, weight and power means higher material cost per unit
	* I kind of need to quantify the cost difference between two smaller CPUs 		+ duplicate peripherals and one slightly bigger CPU with potentially 		shared peripherals

\subsection{Optimal time to market}
In section \ref{optimal-ttm} the concepts of \keyword{attractive and must-be functional requirements}, as well as the\keyword{ideal time to market} have been introduced. Now it will be analyzed how each architecture can impact the time to market and therefore not only the unit cost of the device but also what features are feasible to integrate.

Both architectures have in common, that they can enable the use of unverified third-party or even open-source software for the less critical  components of the device. However, the hypervisor has the advantage that partitions are cheaper to add. This means it is more likely that the cost of adding a partition is outweighed by the benefits of satisfying more attractive functional requirements. Assuming that the license cost of the hypervisor does not negate this benefit for a specific project.

Another way for the architectures to impact the development time is the implementation time of the architecture itself. Not only is the \gls{HSS} architecture typically implemented individually for each project, but some development efforts even have to be duplicated. If the subsystems are heterogeneous the \gls{HAL} will also need to be implemented multiple times. 

Assuming that the hypervisor is developed by a third-party the development efforts of the device manufacturer are reduced. They still need to potentially modify the hypervisor to run on the target platform and configure it but the separation itself is taken care of.

% TODO Find a better way to phrase this
One last type of time saving factor is the impact on the development process itself. These are, however highly dependent on the actual situation and the differences listed here are to be understood as criteria too look out for and not guaranteed benefits of a hypervisor: Oftentimes the software team can't begin work until the hardware team has assembled a first prototype of the device. The hypervisor architecture has both a less complex hardware and significant hardware abstraction. Consequently, software development could begin sooner both in a simulated environment and on the first prototype. Additionally, a hypervisor developer is able to provide a fully fledged \gls{IDE} to make configuration of the architecture easier.

%----------------------------------------------------------------------------------------

\section{As few faults as possible [feasible?] in the end product}
There are two ways for an architecture to influence the faults in a device. First, the architecture itself may be faulty. Second, the architecture may impact the faults in the business software on top of the architecture positively or negatively.

There is a key principle of how a separation architecture might influence faults in the business software running on top. The basic attributes of a separation architecture are similar to the principles of good software architecture design below. Those have been successfully used to reduce faults and manage complexity in software projects over the past decades [source]. Presumably, they have considerable efficacy on the level of device architecture as well. 
% TODO Explain the ones I rely on and cut the ones that don't apply
\begin{enumerate}
\item Separation of concerns
\item Single responsibility principle
\item Principle of least knowledge
\item Don't repeat yourself
\item Minimize upfront design
\end{enumerate}
* There is a key principle a hypervisor might influence faults in the business software 
* It is based on the assumption that hypervisor partitions are cheap enough so it makes sense to separate more than two domains. For example putting each driver in a native API partition.
* The benefits capitalize on the current principles of good software architecture design [List them]
* 
\paragraph{Faults in the architectures themselves}
General solutions, like the hypervisor, are reused over a long period of time and consequently there are more chances to fix faults in the original implementation. For example, faults found by static analyzers in the Linux driver code have decreased significantly over the past 10 years \cite{palix2011faults}. Granted, hypervisors do not enjoy the same popularity as Linux but they also contain much less code and safety hypervisors are used by very scrutinous  parties that are perhaps more likely to find faults.

% NOTE This feels a bit out of place. Maybe a crucial difference or go to safety section (or there is some overlap and I need to decide)
But hypervisors have a significant disadvantage, they represent a single point of failure in the system. This probably has the biggest implications for security but is also very important for safety. For some systems a single point of failure might not be tolerable at all for others the hypervisor's unerring functionality just needs to be sufficiently evidenced. Perhaps even going as far as \keyword{formal verification} [source seL4].

% TODO Include hardware vs software points
[* Hardware vs. software]
* Complexity breeds faults

% NOTE Is "reducing" the best term?
\paragraph{Reducing the frequency of fault implementation}
A k

* Development tools can help. Hypervisor has the ability to provide more sophisticated tools because the system is more restricted and more is known about it. HSS is a very general solution
* If adding partitions in HV is cheap enough it can be used to enforce modularity and prevent fault implementation through the paradigms of software architecture
* HSS might be more complex to develop for (Points mirrored in "Human factors")
* HV could be misconfigured
\paragraph{Finding  faults effectively}
* Hypervisor can potentially support more distinct partitions, which can make finding faults easier. 
* If there are faults that are either in the hypervisor directly or caused by its behavior finding them could be difficult. Depends on the tooling the hypervisor developer offers and whether or not there is source code.
* Hypervisors might allow for a more easily testable product [Why though? Can I back this up?]
* HSS can be more difficult to debug (Points mirrored in "Human factors")
* HSS might have more difficult to replicate bugs
* Ultimately HSS has less control over the entire system at runtime (Which is kind of the point)
* HV can offer debugging and tracing for partitions 
\paragraph{Fixing faults effectively}
* Hypervisor internal faults would probably need the hypervisor dev to fix
* Talk about recertification
\subsection{The lowest possible fault severity}
\paragraph{Fault isolation}
* Both are separation architectures. Hypervisor can potentially establish more partitions and therefore reduce the impact even more.
* HSS has almost perfect separation but reliability may suffer in very large systems
* HV can be seen as a single point of failure. It is fairly small and usually well certified but a single point of failure nevertheless. 
\paragraph{How critical the affected parts are}
% TODO Add this as a subpoint to How much of the system is effected and delete this section. 
* If the components are extremely high risk HSS can be a better choice. Its separation is basically perfect and most likely offers much more control over the critical parts environment
\paragraph{Fault tolerance}
* Hypervisor often offers Health monitoring. Because it has a full view of the system it can monitor dangerous system events and take action.
* HV can not necessarily recognize faulty hardware or "flipped bits". HSS can have a completely separate processors running distinct software that can recognize and maybe mitigate errors of the main processors. This is a typical scenario. HV can only do this through software redundancy (maybe also on another core), which is a debated method.
* 

%----------------------------------------------------------------------------------------

\section{Engaged and satisfied employees}
% TODO Call this "Human factors" and include environmental protection
Ease of use for a hypervisor is mostly dependent on the development tools provided, and therefore hard to generalize. The most sophisticated examples might have a fully fledged \gls{IDE} that aids the configuration of the hypervisor and perhaps even fully integrated debugging and tracing for devices currently running a hypervisor. % TODO add a counter example

% NOTE Too informal, biased
However, hypervisors are still a new technology and especially in such a conservative industry as safety-critical device manufacturing there is bound to be skepticism. This could increase the technological risk on a project using the architecture significantly.

% REWRITE This is now a crucial difference
For \gls{HSS} there is no specialized tooling as it is a unique configuration of hardware components. In fact, typically the subsystems are heterogeneous processors which means they need to be build separately. There are a number of complications with the build, debug and deploy process on these kinds of systems:
\begin{itemize}
\item Binaries have to be compiled differently.
\item Multiple, distinct bootstrapping processes. % TODO I think this also goes somewhere else
\item Separate debugging and tracing mechanisms % TODO Figure out what this could look like
\item More difficult to replicate bugs because only one processor be debugged at a time (kind off)
\item [...]
\end{itemize}
These complications can make finding faults in the code more challenging and generally make life for the developers more difficult.

%----------------------------------------------------------------------------------------

\section{Low risk / Low variance}
* In the conservative realm of safety-critical device manufacturing, the hypervisor is still a new technology and as such regarded with skepticism. "Proven in use" is still an important argument for a technologies safety
* Software solutions ultimately involve more risk than hardware solutions.
* 

%----------------------------------------------------------------------------------------

\section{Future-proof design}
[Might wanna get rid of this "upper section"]

%----------------------------------------------------------------------------------------

\section{Reusable software}
* Hypervisors enable some hardware abstraction because a API is available that can be programmed against and the actual application software is not reliant on the hardware (or HAL) beneath.
* Hypervisor itself is general and reusable (with caveats)
* 

%----------------------------------------------------------------------------------------

\section{Extendibility}
% NOTE I have written this a million times by now
Not only is software more easy to modify than hardware but adding partitions is also cheaper in the hypervisor than in the \gls{HSS} architecture. Therefore, it is conceivable that another partition may be added in a future release, assuming that the necessary computational resources are still available on the platform. 

* If the device is designed to accommodate for
it, it could be possible to simply add another partition with more functionality without touching the others and having to certify them
* If a partition needs to be changed, regression analysis should be as easy as saying: We showed the hypervisor isolates so the other parts can not be affected (Apart from maybe communication channels). Recertification should therefore be significantly simpler and cheaper
* 

%----------------------------------------------------------------------------------------

\section{Maintainability}

%----------------------------------------------------------------------------------------

\section{Why the hypervisor architecture is the focus}
% TODO Rename

* Not quit the same level of safety probably but as far as we know sufficient in most cases. 
* License cost is unclear and depends on the project but if it is cheaper than HSS mixed criticality could be cheaper to integrate
-> Definitely a win in SWaP
* Hypervisor scales better and can therefore support more granular partitions. This can aid modifiability and regression analysis.

* Now the general qualitative differences that can be expected between HSS and HV have been established it has become evident that a hypervisor can add value through more scalable separation and features beyond. As a software solution it can incoroporate many smaller features that would not be possible in HSS.
* Now let's get into some specific examples to highlight when it should best be used and what should be considered when choosing an implementation.
